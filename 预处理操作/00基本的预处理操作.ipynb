{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "内容参考sklearn官方文档preprocessing data 部分\n",
    "数据预处理是打造一个好的模型的重要的前提，常用到的数据处理方法有缩放、\n",
    "归一化、序列化、二值化和缺失值处理等，这里主要以sklearn和pandas作为\n",
    "工具对数据进行操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##编码\n",
    "编码方式常用的方式主要有标签编码（LabelEncoder）和独热编码（OneHotEncoder）\n",
    "\n",
    "标签编码：\n",
    "望文生义，标签编码就是有多少个标签生成多少个编码\n",
    "\n",
    "独热编码：简单地说就是有多少离散特征就有多少比特\n",
    "1）关于为什么要将特征向量映射到欧式空间？\n",
    "一个总要的考量是用独热编码将离散特征扩展到欧式空间，是的距离计算更加合理，因为常用的距离和相似度的计算都是在欧式空间上展开计算的\n",
    "2）优缺点：\n",
    "优点：解决了机器学习中难以处理的属性数据的问题，因为电脑是无法理解属性的，此外一定程度上起到了扩展特征的作用\n",
    "缺点：特征很多的时候会造成特征空间很大，有时候需要降维操作\n",
    "3）需要注意的是很多基于树的算法是不用独热编码的，对于决策树来说独热编码实质上是增加树的深度，就像上面提到的扩展特征的作用。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['no lenses', 'soft', 'no lenses', 'hard', 'no lenses', 'soft', 'no lenses', 'hard', 'no lenses', 'soft', 'no lenses', 'hard', 'no lenses', 'soft', 'no lenses', 'no lenses', 'no lenses', 'no lenses', 'no lenses', 'hard', 'no lenses', 'soft', 'no lenses', 'no lenses']\n",
      "           age astigmatic prescript tearRate\n",
      "0        young         no     myope  reduced\n",
      "1        young         no     myope   normal\n",
      "2        young        yes     myope  reduced\n",
      "3        young        yes     myope   normal\n",
      "4        young         no     hyper  reduced\n",
      "5        young         no     hyper   normal\n",
      "6        young        yes     hyper  reduced\n",
      "7        young        yes     hyper   normal\n",
      "8          pre         no     myope  reduced\n",
      "9          pre         no     myope   normal\n",
      "10         pre        yes     myope  reduced\n",
      "11         pre        yes     myope   normal\n",
      "12         pre         no     hyper  reduced\n",
      "13         pre         no     hyper   normal\n",
      "14         pre        yes     hyper  reduced\n",
      "15         pre        yes     hyper   normal\n",
      "16  presbyopic         no     myope  reduced\n",
      "17  presbyopic         no     myope   normal\n",
      "18  presbyopic        yes     myope  reduced\n",
      "19  presbyopic        yes     myope   normal\n",
      "20  presbyopic         no     hyper  reduced\n",
      "21  presbyopic         no     hyper   normal\n",
      "22  presbyopic        yes     hyper  reduced\n",
      "23  presbyopic        yes     hyper   normal\n",
      "    age  astigmatic  prescript  tearRate\n",
      "0     2           0          1         1\n",
      "1     2           0          1         0\n",
      "2     2           1          1         1\n",
      "3     2           1          1         0\n",
      "4     2           0          0         1\n",
      "5     2           0          0         0\n",
      "6     2           1          0         1\n",
      "7     2           1          0         0\n",
      "8     0           0          1         1\n",
      "9     0           0          1         0\n",
      "10    0           1          1         1\n",
      "11    0           1          1         0\n",
      "12    0           0          0         1\n",
      "13    0           0          0         0\n",
      "14    0           1          0         1\n",
      "15    0           1          0         0\n",
      "16    1           0          1         1\n",
      "17    1           0          1         0\n",
      "18    1           1          1         1\n",
      "19    1           1          1         0\n",
      "20    1           0          0         1\n",
      "21    1           0          0         0\n",
      "22    1           1          0         1\n",
      "23    1           1          0         0\n"
     ]
    }
   ],
   "source": [
    "#基于sklearn的编码\n",
    "#这里采用机器学习实战中隐形眼镜的数据集\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import pandas as pd \n",
    "\n",
    "with open('lenses.txt', 'r') as fr:                                        #加载文件\n",
    "    lenses = [inst.strip().split('\\t') for inst in fr.readlines()]        #处理文件\n",
    "lenses_target = []                                                        #提取每组数据的类别，保存在列表里\n",
    "for each in lenses:\n",
    "    lenses_target.append(each[-1])\n",
    "print(lenses_target)\n",
    "\n",
    "lensesLabels = ['age', 'prescript', 'astigmatic', 'tearRate']            #特征标签       \n",
    "lenses_list = []                                                        #保存lenses数据的临时列表\n",
    "lenses_dict = {}                                                        #保存lenses数据的字典，用于生成pandas\n",
    "for each_label in lensesLabels:                                            #提取信息，生成字典\n",
    "    for each in lenses:\n",
    "        lenses_list.append(each[lensesLabels.index(each_label)])\n",
    "    lenses_dict[each_label] = lenses_list\n",
    "    lenses_list = []\n",
    "# print(lenses_dict)                                             #打印字典信息\n",
    "lenses_pd = pd.DataFrame(lenses_dict)                        #生成pandas.DataFrame\n",
    "lenses_pd_copy=lenses_pd.copy()\n",
    "print(lenses_pd)                                            #打印pandas.DataFrame\n",
    "le = LabelEncoder()                                         #创建LabelEncoder()对象，用于序列化           \n",
    "for col in lenses_pd.columns:                               #序列化\n",
    "    lenses_pd[col] = le.fit_transform(lenses_pd[col])\n",
    "print(lenses_pd)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python35\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 1., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#one-hot\n",
    "le = OneHotEncoder()                                                  \n",
    "le.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])       # fit来学习编码\n",
    "le.transform([[0, 1, 3]]).toarray()                        # 进行编码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "array([[1., 0., 0., 1., 0., 0., 0., 0., 1.]])的理解\n",
    "0 0 3     左边的数据矩阵，第一列为第一个特征维度有两种取值0\\1. 所以对应编码方式为10 、01\n",
    "1 1 0     同理，第二列为第二个特征维度，有三种取值0\\1\\2，所以对应编码方式为100、010、001\n",
    "0 2 1     同理，第三列为第三个特征维度，有四中取值0\\1\\2\\3，所以对应编码方式为1000、0100、           0010、0001\n",
    "1 0 2\n",
    "结果中1,0；对应[0,1,3]中的0第一列只有两个取值\n",
    "0,1,0 对应[0,1,3]中1该列有三个取值\n",
    "0,0,0,1 对应[0,1,3]中的3该列有4个取值\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据标准化,去除均值和方差缩放Standardization or mean removal and variance scaling \n",
    "数据标准化可以实现数据中心化，0均值单位方差 zero mean and unit variance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -1.22474487,  1.33630621],\n",
       "       [ 1.22474487,  0.        , -0.26726124],\n",
       "       [-1.22474487,  1.22474487, -1.06904497]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#标准化之后均值为0 ，方差为1\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "X_train = np.array([[ 1., -1.,  2.],\n",
    "                    [ 2.,  0.,  0.],\n",
    "                    [ 0.,  1., -1.]])\n",
    "X_scaled = preprocessing.scale(X_train)\n",
    "X_scaled                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#检查均值\n",
    "X_scaled.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#检查std\n",
    "X_scaled.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#使用StandardScaler使标准化应用在测试集上：保存标准化参数\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.        , 0.33333333])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.mean_   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -1.22474487,  1.33630621],\n",
       "       [ 1.22474487,  0.        , -0.26726124],\n",
       "       [-1.22474487,  1.22474487, -1.06904497]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.transform(X_train)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.98039216, 0.90196078, 0.97916667])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pepiline也可以实现类似功能\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import svm\n",
    "from sklearn import model_selection\n",
    "from sklearn.datasets import load_iris\n",
    "iris=load_iris()\n",
    "clf = make_pipeline(preprocessing.StandardScaler(), svm.SVC(C=1))\n",
    "model_selection.cross_val_score(clf, iris.data, iris.target, cv=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Scaling features to a range\n",
    " 将特征的取值缩小到一个范围（如0到1）\n",
    " 使用这种缩放的动机包括对非常小的特征标准差的鲁棒性，以及在稀疏数矩阵中保持零项。 \n",
    " 两个常用方法：MinMaxScaler or MaxAbsScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5       , 0.        , 1.        ],\n",
       "       [1.        , 0.5       , 0.33333333],\n",
       "       [0.        , 1.        , 0.        ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MinMaxScaler方法（x-min）/max\n",
    "X_train = np.array([[ 1., -1.,  2.],\n",
    "                    [ 2.,  0.,  0.],\n",
    "                    [ 0.,  1., -1.]])\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_train_minmax = min_max_scaler.fit_transform(X_train)\n",
    "X_train_minmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5, -1. ,  1. ],\n",
       "       [ 1. ,  0. ,  0. ],\n",
       "       [ 0. ,  1. , -0.5]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MaxAbsScaler方法 x/max\n",
    "X_train = np.array([[ 1., -1.,  2.],\n",
    "                    [ 2.,  0.,  0.],\n",
    "                    [ 0.,  1., -1.]])\n",
    "\n",
    "max_abs_scaler = preprocessing.MaxAbsScaler()\n",
    "X_train_maxabs = max_abs_scaler.fit_transform(X_train)\n",
    "X_train_maxabs                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Non-linear transformation\n",
    " 非线性转化提供两种方法：QuantileTransformer and quantile_transform（幂转换） \n",
    " 分位数变换和幂变换都是基于特征的单调变换，从而保持每个特征值的秩\n",
    " 分位变换在公式G−1(f(X)的基础上，将所有特征放入相同的期望分布中，其中f是特征的累积分布函数，G−1是期望输出分布G的分位函数。公式条件：(1）如果x是一个具有连续累积分布函数f的随机变量，那么f(X)在[0，1]上均匀分布；(2)如果u是[0，1]上分布均匀的随机变量，则G−1(U)具有G分布。通过进行秩变换，分位数变换消除了不寻常分布，比缩放方法更少受异常值的影响。然而，它确实扭曲了特征内部和之间的相关性和距离。\n",
    "幂变换是一类参数变换，其目的是将数据从任意分布映射到接近高斯分布的位置。 \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python35\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2239: UserWarning: n_quantiles (1000) is greater than the total number of samples (112). n_quantiles is set to n_samples.\n",
      "  % (self.n_quantiles, n_samples))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([4.3, 5.1, 5.8, 6.5, 7.9])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#映射到均匀分布\n",
    "'''QuantileTransformer and quantile_transform\n",
    "提供了一种非参数转换，将数据映射到值介于0到1之间的均匀分布\n",
    "'''\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "#preprocessing.QuantileTransformer\n",
    "quantile_transformer = preprocessing.QuantileTransformer(random_state=0)\n",
    "X_train_trans = quantile_transformer.fit_transform(X_train)\n",
    "X_test_trans = quantile_transformer.transform(X_test)\n",
    "\"\"\"就是计算X_train[:,0]d第一列排序后的的[0,25,50,75,100]%数\"\"\"\n",
    "np.percentile(X_train[:, 0], [0, 25, 50, 75, 100]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.54954955, 0.43693694, 0.48198198, 0.59009009],\n",
       "       [0.50900901, 0.16216216, 0.42342342, 0.41441441],\n",
       "       [0.85135135, 0.43693694, 0.80630631, 0.85135135],\n",
       "       [0.06756757, 0.66666667, 0.04504505, 0.13513514],\n",
       "       [0.87837838, 0.57657658, 0.71621622, 0.93243243],\n",
       "       [0.17567568, 0.86936937, 0.27927928, 0.32432432],\n",
       "       [0.31981982, 0.91441441, 0.1981982 , 0.13513514],\n",
       "       [0.17567568, 0.        , 0.35135135, 0.35585586],\n",
       "       [0.74324324, 0.43693694, 0.80630631, 0.7027027 ],\n",
       "       [0.80630631, 0.74774775, 0.86486486, 1.        ],\n",
       "       [0.57657658, 0.01351351, 0.67117117, 0.59009009],\n",
       "       [0.80630631, 0.11261261, 0.89189189, 0.7027027 ],\n",
       "       [0.4009009 , 0.11261261, 0.4009009 , 0.38738739],\n",
       "       [0.97747748, 0.43693694, 0.94144144, 0.93243243],\n",
       "       [0.65315315, 0.74774775, 0.5990991 , 0.63063063],\n",
       "       [0.36036036, 0.06306306, 0.38738739, 0.38738739],\n",
       "       [0.65315315, 0.20720721, 0.63963964, 0.7027027 ],\n",
       "       [0.65315315, 0.26576577, 0.71621622, 0.59009009],\n",
       "       [0.12612613, 0.11261261, 0.55405405, 0.65315315],\n",
       "       [0.65315315, 0.11261261, 0.67117117, 0.75675676],\n",
       "       [0.9009009 , 0.66666667, 0.5990991 , 0.54504505],\n",
       "       [0.74324324, 0.43693694, 0.75225225, 0.8018018 ],\n",
       "       [0.57657658, 0.81081081, 0.55405405, 0.63063063],\n",
       "       [0.09009009, 0.57657658, 0.27927928, 0.13513514],\n",
       "       [0.50900901, 0.20720721, 0.71621622, 0.75675676],\n",
       "       [0.4009009 , 0.20720721, 0.48198198, 0.47747748],\n",
       "       [0.4009009 , 0.31531532, 0.36936937, 0.47747748],\n",
       "       [0.36036036, 0.11261261, 0.42342342, 0.47747748],\n",
       "       [0.59459459, 0.43693694, 0.58108108, 0.54504505],\n",
       "       [0.92792793, 0.66666667, 0.92792793, 0.7027027 ],\n",
       "       [0.2972973 , 0.91441441, 0.1981982 , 0.13513514],\n",
       "       [0.        , 0.43693694, 0.        , 0.        ],\n",
       "       [0.7027027 , 0.20720721, 0.77027027, 0.75675676],\n",
       "       [0.45045045, 0.43693694, 0.48198198, 0.41441441],\n",
       "       [0.31981982, 0.81081081, 0.31531532, 0.13513514],\n",
       "       [0.45045045, 1.        , 0.1981982 , 0.2972973 ],\n",
       "       [0.87837838, 0.57657658, 0.63963964, 0.59009009],\n",
       "       [0.04504505, 0.57657658, 0.1981982 , 0.13513514],\n",
       "       [0.54954955, 0.43693694, 0.71621622, 0.7027027 ],\n",
       "       [0.23873874, 0.11261261, 0.33333333, 0.38738739],\n",
       "       [0.04504505, 0.81081081, 0.10810811, 0.25675676],\n",
       "       [0.60810811, 0.01351351, 0.55405405, 0.59009009],\n",
       "       [0.92792793, 0.8963964 , 0.94144144, 1.        ],\n",
       "       [0.45045045, 0.31531532, 0.48198198, 0.47747748],\n",
       "       [0.09009009, 0.43693694, 0.10810811, 0.        ],\n",
       "       [0.90990991, 0.43693694, 0.91441441, 0.85135135],\n",
       "       [0.87837838, 0.66666667, 0.86486486, 0.93243243],\n",
       "       [0.74324324, 0.43693694, 0.89189189, 0.88738739],\n",
       "       [0.7027027 , 0.26576577, 0.83333333, 0.85135135],\n",
       "       [0.23873874, 0.95045045, 0.27927928, 0.13513514],\n",
       "       [0.09009009, 0.81081081, 0.27927928, 0.13513514],\n",
       "       [0.74324324, 0.66666667, 0.71621622, 0.8018018 ],\n",
       "       [0.80630631, 0.74774775, 0.86486486, 0.85135135],\n",
       "       [0.02702703, 0.03603604, 0.04504505, 0.25675676],\n",
       "       [0.60810811, 0.81081081, 0.78828829, 0.93243243],\n",
       "       [0.12612613, 0.43693694, 0.10810811, 0.13513514],\n",
       "       [0.45045045, 0.11261261, 0.67117117, 0.8018018 ],\n",
       "       [0.87837838, 0.57657658, 0.78828829, 0.85135135],\n",
       "       [0.01351351, 0.66666667, 0.04504505, 0.13513514],\n",
       "       [0.17567568, 0.8963964 , 0.10810811, 0.13513514],\n",
       "       [0.92792793, 0.43693694, 0.89189189, 0.63063063],\n",
       "       [0.23873874, 0.86936937, 0.10810811, 0.25675676],\n",
       "       [0.01351351, 0.43693694, 0.04504505, 0.13513514],\n",
       "       [0.31981982, 0.97297297, 0.31531532, 0.2972973 ],\n",
       "       [0.36036036, 0.03603604, 0.42342342, 0.47747748],\n",
       "       [0.85135135, 0.66666667, 0.91441441, 0.93243243],\n",
       "       [0.95495495, 0.43693694, 0.97297297, 0.85135135],\n",
       "       [0.23873874, 0.86936937, 0.10810811, 0.13513514],\n",
       "       [0.12612613, 0.57657658, 0.1981982 , 0.13513514],\n",
       "       [0.28828829, 0.81081081, 0.10810811, 0.13513514],\n",
       "       [0.45045045, 0.26576577, 0.55405405, 0.47747748],\n",
       "       [0.77027027, 0.43693694, 0.52252252, 0.54504505],\n",
       "       [0.17567568, 0.66666667, 0.01351351, 0.13513514],\n",
       "       [0.23873874, 0.74774775, 0.31531532, 0.31531532],\n",
       "       [0.7027027 , 0.31531532, 0.5045045 , 0.47747748],\n",
       "       [0.31981982, 0.81081081, 0.1981982 , 0.2972973 ],\n",
       "       [0.97747748, 0.16216216, 1.        , 0.93243243],\n",
       "       [0.12612613, 0.06306306, 0.34234234, 0.35585586],\n",
       "       [1.        , 0.95045045, 0.96396396, 0.8018018 ],\n",
       "       [0.80630631, 0.57657658, 0.52252252, 0.54504505],\n",
       "       [0.28828829, 0.99099099, 0.1981982 , 0.        ],\n",
       "       [0.57657658, 0.43693694, 0.61711712, 0.7027027 ],\n",
       "       [0.50900901, 0.98198198, 0.01351351, 0.13513514],\n",
       "       [0.97747748, 0.26576577, 0.98648649, 0.8018018 ],\n",
       "       [0.23873874, 0.95045045, 0.1981982 , 0.25675676],\n",
       "       [0.06756757, 0.66666667, 0.27927928, 0.13513514],\n",
       "       [0.94594595, 0.26576577, 0.94144144, 0.75675676],\n",
       "       [0.17567568, 0.74774775, 0.10810811, 0.13513514],\n",
       "       [0.65315315, 0.81081081, 0.83333333, 0.97747748],\n",
       "       [0.45045045, 0.26576577, 0.45045045, 0.47747748],\n",
       "       [0.50900901, 0.20720721, 0.4009009 , 0.41441441],\n",
       "       [0.45045045, 0.16216216, 0.35135135, 0.35585586],\n",
       "       [0.7027027 , 0.66666667, 0.77027027, 0.93243243],\n",
       "       [0.80630631, 0.43693694, 0.75225225, 0.93243243],\n",
       "       [0.65315315, 0.11261261, 0.63963964, 0.59009009],\n",
       "       [0.80630631, 0.43693694, 0.67117117, 0.65315315],\n",
       "       [0.17567568, 0.43693694, 0.27927928, 0.13513514],\n",
       "       [0.36036036, 0.06306306, 0.37837838, 0.35585586],\n",
       "       [0.80630631, 0.57657658, 0.83333333, 0.97747748],\n",
       "       [0.50900901, 0.20720721, 0.71621622, 0.75675676],\n",
       "       [0.23873874, 0.81081081, 0.1981982 , 0.13513514],\n",
       "       [0.77027027, 0.31531532, 0.58108108, 0.47747748],\n",
       "       [0.4009009 , 0.43693694, 0.45045045, 0.47747748],\n",
       "       [0.54954955, 0.66666667, 0.61711712, 0.7027027 ],\n",
       "       [0.65315315, 0.03603604, 0.52252252, 0.47747748],\n",
       "       [0.36036036, 0.86936937, 0.04504505, 0.13513514],\n",
       "       [0.23873874, 0.91441441, 0.1981982 , 0.2972973 ],\n",
       "       [0.12612613, 0.57657658, 0.1981982 , 0.        ],\n",
       "       [0.65315315, 0.31531532, 0.83333333, 0.7027027 ],\n",
       "       [0.50900901, 0.20720721, 0.45045045, 0.35585586],\n",
       "       [0.97747748, 0.95045045, 0.98648649, 0.88738739],\n",
       "       [0.04504505, 0.66666667, 0.10810811, 0.13513514]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#分位数转化之后\n",
    "X_train_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.23873874, 0.50900901, 0.74324324, 1.        ])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#结果就是一边会无限接近0，另一端无限接近1 \n",
    "np.percentile(X_train_trans[:, 0], [0, 25, 50, 75, 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "幂转换提供两种方式yeo-Johnson变换和box-cox变换。\n",
    "其中yeo-Johnson变换\n",
    "$$\\begin{split}x_i^{(\\lambda)} =\n",
    "\\begin{cases}\n",
    " [(x_i + 1)^\\lambda - 1] / \\lambda & \\text{if } \\lambda \\neq 0, x_i \\geq 0, \\\\[8pt]\n",
    "\\ln{(x_i) + 1} & \\text{if } \\lambda = 0, x_i \\geq 0 \\\\[8pt]\n",
    "-[(-x_i + 1)^{2 - \\lambda} - 1] / (2 - \\lambda) & \\text{if } \\lambda \\neq 2, x_i < 0, \\\\[8pt]\n",
    " - \\ln (- x_i + 1) & \\text{if } \\lambda = 2, x_i < 0\n",
    "\\end{cases}\\end{split}$$\n",
    "\n",
    "其中box-cox变换\n",
    "$$\\begin{split}x_i^{(\\lambda)} =\n",
    "\\begin{cases}\n",
    "\\dfrac{x_i^\\lambda - 1}{\\lambda} & \\text{if } \\lambda \\neq 0, \\\\[8pt]\n",
    "\\ln{(x_i)} & \\text{if } \\lambda = 0,\n",
    "\\end{cases}\\end{split}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.28331718, 1.18092228, 0.84160269],\n",
       "       [0.94293279, 1.60960836, 0.3879099 ],\n",
       "       [1.35235668, 0.21715673, 1.09977091]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Mapping to a Gaussian distribution\n",
    "映射到高斯分布\n",
    "'''\n",
    "from sklearn import  preprocessing\n",
    "import numpy as np\n",
    "\n",
    "pt = preprocessing.PowerTransformer(method='box-cox', standardize=False)\n",
    "'''lognormal对数正态分布\n",
    "数正态分布（logarithmic normal distribution）是指一个随机变量的对数服从正态分布，\n",
    "则该随机变量服从对数正态分布。对数正态分布从短期来看，与正态分布非常接近\n",
    "但长期来看，对数正态分布向上分布的数值更多一些。\n",
    "'''\n",
    "X_lognormal = np.random.RandomState(616).lognormal(size=(3, 3))\n",
    "X_lognormal                                         \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.49024349,  0.17881995, -0.1563781 ],\n",
       "       [-0.05102892,  0.58863195, -0.57612414],\n",
       "       [ 0.69420009, -0.84857822,  0.10051454]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "result=pt.fit_transform(X_lognormal)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python35\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2239: UserWarning: n_quantiles (1000) is greater than the total number of samples (150). n_quantiles is set to n_samples.\n",
      "  % (self.n_quantiles, n_samples))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[4.3, 2. , 1. , 0.1],\n",
       "       [4.4, 2.2, 1.1, 0.1],\n",
       "       [4.4, 2.2, 1.2, 0.1],\n",
       "       [4.4, 2.2, 1.2, 0.1],\n",
       "       [4.5, 2.3, 1.3, 0.1],\n",
       "       [4.6, 2.3, 1.3, 0.2],\n",
       "       [4.6, 2.3, 1.3, 0.2],\n",
       "       [4.6, 2.3, 1.3, 0.2],\n",
       "       [4.6, 2.4, 1.3, 0.2],\n",
       "       [4.7, 2.4, 1.3, 0.2],\n",
       "       [4.7, 2.4, 1.3, 0.2],\n",
       "       [4.8, 2.5, 1.4, 0.2],\n",
       "       [4.8, 2.5, 1.4, 0.2],\n",
       "       [4.8, 2.5, 1.4, 0.2],\n",
       "       [4.8, 2.5, 1.4, 0.2],\n",
       "       [4.8, 2.5, 1.4, 0.2],\n",
       "       [4.9, 2.5, 1.4, 0.2],\n",
       "       [4.9, 2.5, 1.4, 0.2],\n",
       "       [4.9, 2.5, 1.4, 0.2],\n",
       "       [4.9, 2.6, 1.4, 0.2],\n",
       "       [4.9, 2.6, 1.4, 0.2],\n",
       "       [4.9, 2.6, 1.4, 0.2],\n",
       "       [5. , 2.6, 1.4, 0.2],\n",
       "       [5. , 2.6, 1.4, 0.2],\n",
       "       [5. , 2.7, 1.5, 0.2],\n",
       "       [5. , 2.7, 1.5, 0.2],\n",
       "       [5. , 2.7, 1.5, 0.2],\n",
       "       [5. , 2.7, 1.5, 0.2],\n",
       "       [5. , 2.7, 1.5, 0.2],\n",
       "       [5. , 2.7, 1.5, 0.2],\n",
       "       [5. , 2.7, 1.5, 0.2],\n",
       "       [5. , 2.7, 1.5, 0.2],\n",
       "       [5.1, 2.7, 1.5, 0.2],\n",
       "       [5.1, 2.8, 1.5, 0.2],\n",
       "       [5.1, 2.8, 1.5, 0.3],\n",
       "       [5.1, 2.8, 1.5, 0.3],\n",
       "       [5.1, 2.8, 1.5, 0.3],\n",
       "       [5.1, 2.8, 1.6, 0.3],\n",
       "       [5.1, 2.8, 1.6, 0.3],\n",
       "       [5.1, 2.8, 1.6, 0.3],\n",
       "       [5.1, 2.8, 1.6, 0.3],\n",
       "       [5.2, 2.8, 1.6, 0.4],\n",
       "       [5.2, 2.8, 1.6, 0.4],\n",
       "       [5.2, 2.8, 1.6, 0.4],\n",
       "       [5.2, 2.8, 1.7, 0.4],\n",
       "       [5.3, 2.8, 1.7, 0.4],\n",
       "       [5.4, 2.8, 1.7, 0.4],\n",
       "       [5.4, 2.9, 1.7, 0.4],\n",
       "       [5.4, 2.9, 1.9, 0.5],\n",
       "       [5.4, 2.9, 1.9, 0.6],\n",
       "       [5.4, 2.9, 3. , 1. ],\n",
       "       [5.4, 2.9, 3.3, 1. ],\n",
       "       [5.5, 2.9, 3.3, 1. ],\n",
       "       [5.5, 2.9, 3.5, 1. ],\n",
       "       [5.5, 2.9, 3.5, 1. ],\n",
       "       [5.5, 2.9, 3.6, 1. ],\n",
       "       [5.5, 2.9, 3.7, 1. ],\n",
       "       [5.5, 3. , 3.8, 1.1],\n",
       "       [5.5, 3. , 3.9, 1.1],\n",
       "       [5.6, 3. , 3.9, 1.1],\n",
       "       [5.6, 3. , 3.9, 1.2],\n",
       "       [5.6, 3. , 4. , 1.2],\n",
       "       [5.6, 3. , 4. , 1.2],\n",
       "       [5.6, 3. , 4. , 1.2],\n",
       "       [5.6, 3. , 4. , 1.2],\n",
       "       [5.7, 3. , 4. , 1.3],\n",
       "       [5.7, 3. , 4.1, 1.3],\n",
       "       [5.7, 3. , 4.1, 1.3],\n",
       "       [5.7, 3. , 4.1, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.3],\n",
       "       [5.8, 3. , 4.3, 1.3],\n",
       "       [5.8, 3. , 4.3, 1.3],\n",
       "       [5.8, 3. , 4.4, 1.3],\n",
       "       [5.8, 3. , 4.4, 1.3],\n",
       "       [5.8, 3. , 4.4, 1.3],\n",
       "       [5.8, 3. , 4.4, 1.4],\n",
       "       [5.8, 3. , 4.5, 1.4],\n",
       "       [5.9, 3. , 4.5, 1.4],\n",
       "       [5.9, 3. , 4.5, 1.4],\n",
       "       [5.9, 3. , 4.5, 1.4],\n",
       "       [6. , 3.1, 4.5, 1.4],\n",
       "       [6. , 3.1, 4.5, 1.4],\n",
       "       [6. , 3.1, 4.5, 1.4],\n",
       "       [6. , 3.1, 4.5, 1.5],\n",
       "       [6. , 3.1, 4.6, 1.5],\n",
       "       [6. , 3.1, 4.6, 1.5],\n",
       "       [6.1, 3.1, 4.6, 1.5],\n",
       "       [6.1, 3.1, 4.7, 1.5],\n",
       "       [6.1, 3.1, 4.7, 1.5],\n",
       "       [6.1, 3.1, 4.7, 1.5],\n",
       "       [6.1, 3.1, 4.7, 1.5],\n",
       "       [6.1, 3.2, 4.7, 1.5],\n",
       "       [6.2, 3.2, 4.8, 1.5],\n",
       "       [6.2, 3.2, 4.8, 1.5],\n",
       "       [6.2, 3.2, 4.8, 1.5],\n",
       "       [6.2, 3.2, 4.8, 1.6],\n",
       "       [6.3, 3.2, 4.9, 1.6],\n",
       "       [6.3, 3.2, 4.9, 1.6],\n",
       "       [6.3, 3.2, 4.9, 1.6],\n",
       "       [6.3, 3.2, 4.9, 1.7],\n",
       "       [6.3, 3.2, 4.9, 1.7],\n",
       "       [6.3, 3.2, 5. , 1.8],\n",
       "       [6.3, 3.2, 5. , 1.8],\n",
       "       [6.3, 3.2, 5. , 1.8],\n",
       "       [6.3, 3.3, 5. , 1.8],\n",
       "       [6.4, 3.3, 5.1, 1.8],\n",
       "       [6.4, 3.3, 5.1, 1.8],\n",
       "       [6.4, 3.3, 5.1, 1.8],\n",
       "       [6.4, 3.3, 5.1, 1.8],\n",
       "       [6.4, 3.3, 5.1, 1.8],\n",
       "       [6.4, 3.4, 5.1, 1.8],\n",
       "       [6.4, 3.4, 5.1, 1.8],\n",
       "       [6.5, 3.4, 5.1, 1.8],\n",
       "       [6.5, 3.4, 5.2, 1.9],\n",
       "       [6.5, 3.4, 5.2, 1.9],\n",
       "       [6.5, 3.4, 5.3, 1.9],\n",
       "       [6.5, 3.4, 5.3, 1.9],\n",
       "       [6.6, 3.4, 5.4, 1.9],\n",
       "       [6.6, 3.4, 5.4, 2. ],\n",
       "       [6.7, 3.4, 5.5, 2. ],\n",
       "       [6.7, 3.4, 5.5, 2. ],\n",
       "       [6.7, 3.4, 5.5, 2. ],\n",
       "       [6.7, 3.5, 5.6, 2. ],\n",
       "       [6.7, 3.5, 5.6, 2. ],\n",
       "       [6.7, 3.5, 5.6, 2.1],\n",
       "       [6.7, 3.5, 5.6, 2.1],\n",
       "       [6.7, 3.5, 5.6, 2.1],\n",
       "       [6.8, 3.5, 5.6, 2.1],\n",
       "       [6.8, 3.6, 5.7, 2.1],\n",
       "       [6.8, 3.6, 5.7, 2.1],\n",
       "       [6.9, 3.6, 5.7, 2.2],\n",
       "       [6.9, 3.6, 5.8, 2.2],\n",
       "       [6.9, 3.7, 5.8, 2.2],\n",
       "       [6.9, 3.7, 5.8, 2.3],\n",
       "       [7. , 3.7, 5.9, 2.3],\n",
       "       [7.1, 3.8, 5.9, 2.3],\n",
       "       [7.2, 3.8, 6. , 2.3],\n",
       "       [7.2, 3.8, 6. , 2.3],\n",
       "       [7.2, 3.8, 6.1, 2.3],\n",
       "       [7.3, 3.8, 6.1, 2.3],\n",
       "       [7.4, 3.8, 6.1, 2.3],\n",
       "       [7.6, 3.9, 6.3, 2.4],\n",
       "       [7.7, 3.9, 6.4, 2.4],\n",
       "       [7.7, 4. , 6.6, 2.4],\n",
       "       [7.7, 4.1, 6.7, 2.5],\n",
       "       [7.7, 4.2, 6.7, 2.5],\n",
       "       [7.9, 4.4, 6.9, 2.5]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''还可以通过设置Output_Distribution=‘Normal’，\n",
    "使用Quantile转换器将数据映射到正态分布'''\n",
    "quantile_transformer = preprocessing.QuantileTransformer(\n",
    "                        output_distribution='normal', random_state=0)\n",
    "X_trans = quantile_transformer.fit_transform(X)\n",
    "quantile_transformer.quantiles_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.40824829, -0.40824829,  0.81649658],\n",
       "       [ 1.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.70710678, -0.70710678]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#正则化Normalization\n",
    "'''\n",
    "正则化是对单个样本进行定标，使其具有单位范数的过程。\n",
    "如果您计划使用二次型(如点积或任何其他内核)来量化任意一对样本的相似性，\n",
    "则此过程可能非常有用。这种假设是文本分类和聚类环境中常用的向量空间模型的基础。\n",
    "函数Normalize提供了一种在单个类似数组的数据集上执行此操作的快速、\n",
    "简便的方法，无论是使用l1或l2规范： '''\n",
    "X = [[ 1., -1.,  2.],\n",
    "    [ 2.,  0.,  0.],\n",
    "    [ 0.,  1., -1.]]\n",
    "X_normalized = preprocessing.normalize(X, norm='l2')\n",
    "X_normalized                                      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Normalizer(copy=True, norm='l2')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizer = preprocessing.Normalizer().fit(X)  # fit does nothing\n",
    "normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.40824829, -0.40824829,  0.81649658],\n",
       "       [ 1.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.70710678, -0.70710678]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizer.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Discretization离散化\n",
    "离散化(也称为量化或绑定)提供了一种将连续特征划分为离散值的方法。某些具有连续特征的数据集可能受益于离散化，因为离散化可以将连续属性的数据集转换为仅具有名义属性的数据集。一个热编码的离散特征可以使模型更有表现力，同时保持可解释性。\n",
    "这里主要说明两种方法\n",
    "K-bins discretization和Feature binarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [2., 0., 0.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' K-bins discretization\n",
    "k-离散化'''\n",
    "X = np.array([[ -3., 5., 15 ],\n",
    "            [  0., 6., 14 ],\n",
    "            [  6., 3., 11 ]])\n",
    "est = preprocessing.KBinsDiscretizer(n_bins=[3, 2, 2], encode='ordinal').fit(X)\n",
    "est.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Binarizer(copy=True, threshold=0.0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Feature binarization\n",
    "特征二分法'''\n",
    "X = [[ 1., -1.,  2.],\n",
    "    [ 2.,  0.,  0.],\n",
    "    [ 0.,  1., -1.]]\n",
    "\n",
    "binarizer = preprocessing.Binarizer().fit(X)  # fit does nothing\n",
    "binarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binarizer.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#设置阈值\n",
    "binarizer=preprocessing.Binarizer(threshold=1.1)\n",
    "binarizer.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于缺省值的除一般用pandas来处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2\n",
       "0  1.0  6.5  3.0\n",
       "1  1.0  0.0  0.0\n",
       "2  0.0  2.0  0.0\n",
       "3  2.0  3.0  4.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import nan as NA \n",
    "import pandas as pd\n",
    "data=pd.DataFrame([[1,6.5,3],[1,NA,NA],[NA,2,NA],[2,3,4]])\n",
    "data1=data.copy()\n",
    "data1.fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2\n",
       "0  1.0  6.5  3.0\n",
       "1  1.0  2.0  3.0\n",
       "2  1.0  2.0  3.0\n",
       "3  2.0  3.0  4.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.fillna({0:1,1:2,2:3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [2, 3],\n",
       "       [4, 5]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#生成多项式特征\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "X = np.arange(6).reshape(3, 2)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  1.,  0.,  0.,  1.],\n",
       "       [ 1.,  2.,  3.,  4.,  6.,  9.],\n",
       "       [ 1.,  4.,  5., 16., 20., 25.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly=PolynomialFeatures(2)\n",
    "poly.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "包括特征$$(1, X_1, X_2, X_1^2, X_1X_2, X_2^2)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.69314718],\n",
       "       [1.09861229, 1.38629436]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#定制转换器\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "transformer=FunctionTransformer(np.log1p,validate=True)\n",
    "X=np.array([[0,1],[2,3]])  \n",
    "transformer.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
